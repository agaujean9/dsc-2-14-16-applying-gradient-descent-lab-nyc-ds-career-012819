{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Gradient Descent - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lesson, we derived the functions that we help us descend along our cost functions efficiently.  Remember that this technique is not so different from what we saw with using the derivative to tell us our next step size and direction in two dimensions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./slopes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When descending along our cost curve in two dimensions, we used the slope of the tangent line at each point, to tell us how large of a step to take next.  And with the our cost curve being a function of $m$ and $b$, we had to use the gradient to determine each step.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./gradientdescent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But really it's an analogous approach.  Just like we can calculate the use derivative of a function $f(x)$ to calculate the slope at a given value of $x$ on the graph, and thus our next step.  Here, we calculated the partial derivative with respect to both variables, our slope and y-intercept, to calculate the amount to move next in either direction, and thus to steer us towards our minimum.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be able to:\n",
    "- Create a full gradient descent algorithm\n",
    "- Apply a gradient descent algorithm on a data set with more than one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviewing our gradient descent formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily for us, we already did the hard work of deriving these formulas.  Now we get to see the fruit of our labor.  The following formulas tell us how to update regression variables of $m$ and $b$ to approach a \"best fit\" line.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $ \\frac{dJ}{dm}J(m,b) = -2\\sum_{i = 1}^n x_i(y_i - (mx_i + b)) = -2\\sum_{i = 1}^n x_i*\\epsilon_i$ \n",
    "* $ \\frac{dJ}{db}J(m,b) = -2\\sum_{i = 1}^n(y_i - (mx_i + b)) = -2\\sum_{i = 1}^n \\epsilon_i $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the formulas above tell us to take some dataset, with values of $x$ and $y$, and then given a regression formula with values $m$ and $b$, iterate through our dataset, and use the formulas to calculate an update to $m$ and $b$.  So ultimately, to descend along the cost function, we will use the calculations:\n",
    "\n",
    "`current_m` = `old_m` $ -  (-2*\\sum_{i=1}^n x_i*\\epsilon_i )$\n",
    "\n",
    "`current_b` =  `old_b` $ - ( -2*\\sum_{i=1}^n \\epsilon_i )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok let's turn this into code.  First, let's initialize our data like we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(225)\n",
    "\n",
    "x = np.random.rand(30, 1).reshape(30)\n",
    "y_randterm = np.random.normal(0,3,30)\n",
    "y = 3 + 50* x + y_randterm\n",
    "\n",
    "data = np.array([y, x])\n",
    "data = np.transpose(data)\n",
    "\n",
    "plt.plot(x, y, '.b')\n",
    "plt.xlabel(\"x\", fontsize=14)\n",
    "plt.ylabel(\"y\", fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now\n",
    "\n",
    "- Let's set our initial regression line by initializing $m$ and $b$ variables as zero.  Store them in `b_current` and `m_current`.\n",
    "- Let's next initialize updates to these variables by setting to variables, `update_to_b` and `update_to_m` equal to 0.\n",
    "- Define an `error_at` function which outputs the error $\\epsilon_i$ for a given $i$. inputs are a row of the particular data set, $b$ and $m$.\n",
    "- Them, use this `error_at` function to iterate through each of the points in the dataset, and at each iteration change our `update_to_b` by $2*\\epsilon$ and change our `update_to_m` by $2*x*\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial variables of our regression line\n",
    "b_current = 0 \n",
    "m_current = 0 \n",
    "\n",
    "\n",
    "#amount to update our variables for our next step\n",
    "update_to_b = 0 \n",
    "update_to_m = 0 \n",
    "\n",
    "# Define the error_at function\n",
    "def error_at(x,b,m): \n",
    "    return (x[0] - (m * x[1] + b))\n",
    "\n",
    "# iterate through data to change update_to_b and update_to_m\n",
    "for i in range(0,len(data)): \n",
    "    update_to_b += -2 * (error_at(data[i], b_current, m_current))\n",
    "    update_to_m += -2 * (error_at(data[i], b_current, m_current))*data[i][1]\n",
    "\n",
    "# Create new_b and new_m by subtracting the updates from the current estimates\n",
    "new_b = b_current - update_to_b\n",
    "new_m = m_current - update_to_m\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last two lines of the code above, we calculate our `new_b` and `new_m` values by updating our taking our current values and adding our respective updates.  We define a function called `error_at`, which we can use in the error component of our partial derivatives above.\n",
    "\n",
    "The code above represents **just one** update to our regression line, and therefore just one step towards our best fit line.  We'll just repeat the process to take multiple steps.  But first we have to make a couple other changes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweaking our approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the above code is very close to what we want, but we just need to make tweaks to our code before it's perfect.\n",
    "\n",
    "The first one is obvious if we think about what these formulas are really telling us to do.  Look at the graph below, and think about what it means to change each of our $m$ and $b$ variables by at least the sum of all of the errors, of the $y$ values that our regression line predicts and our actual data.  That would be an enormous change.  To ensure that we drastically updating our regression line with each step, we multiply each of these partial derivatives by a learning rate.  As we have seen before, the learning rate is just a small number, like $.\n",
    "01$ which controls for how large our updates to the regression line will be.  The learning rate is  represented by the Greek letter eta, $\\eta$, or alpha $\\alpha$.  We'll use eta, so $\\eta = .01$ means the learning rate is $.01$.\n",
    "\n",
    "Multiplying our step size by our learning rate works fine, so long as we multiply both of the partial derivatives by the same amount.  This is because with out gradient,  $ \\nabla J(m,b)$, we think of as steering us in the correct direction.  In other words, our derivatives ensure we are make the correct **proportional** changes to $m$ and $b$.  So scaling down these changes to make sure we don't update our regression line too quickly works fine, so long as we keep me moving in the correct direction.  While were at it, we can also get rid of multiplying our partials by 2.  As mentioned, so long as our changes are proportional we're in good shape. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our second tweak, note that in general the larger the dataset, the larger the sum of our errors would be.  But that doesn't mean our formulas are less accurate, and there deserve larger changes.  It just means that the total error is larger.  But we should really think accuracy as being proportional to the size of our dataset.  We can correct for this effect by dividing the effect of our update by the size of our dataset, $n$.\n",
    "\n",
    "Make these changes below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#amount to update our variables for our next step\n",
    "update_to_b = 0 \n",
    "update_to_m = 0 \n",
    "\n",
    "# define learning rate and n\n",
    "learning_rate = .01 \n",
    "n = len(data) \n",
    "\n",
    "# create update_to_b and update_to_m\n",
    "for i in range(0,n): \n",
    "    update_to_b += -(1/n) * (error_at(data[i], b_current, m_current))\n",
    "    update_to_m += -(1/n) *(error_at(data[i], b_current, m_current) * data[i][0])\n",
    "    \n",
    "# create new_b and new_m\n",
    "new_b = b_current - (learning_rate * update_to_b)\n",
    "new_m = m_current - (learning_rate * update_to_m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our code now reflects what we know about our gradient descent process.  Start with an initial regression line with values of $m$ and $b$.  Then for each point, calculate how the regression line fares against the actual point (that is, find the error).  Update what our next step to the respective variable should be using by using the partial derivative.  And after iterating through all of the points, update the value of $b$ and $m$ appropriately, scaled down by a learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeing our gradient descent formulas in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, the code above represents just one update to our regression line, and therefore just one step towards our best fit line.  To take multiple steps we wrap the process we want to duplicate in a function called `step_gradient` and then can call that function as much as we want. In what's next:\n",
    "\n",
    "- Let's make sure to include a learning_rate of 0.1\n",
    "- Let's output new_b and new_m as list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(b_current, m_current, points):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    learning_rate = .1\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i][1]\n",
    "        y = points[i][0]\n",
    "        b_gradient += -(1/N) * (y - (m_current * x + b_current))\n",
    "        m_gradient += -(1/N) * x * (y -  (m_current * x + b_current))\n",
    "    new_b = b_current - (learning_rate * b_gradient)\n",
    "    new_m = m_current - (learning_rate * m_gradient)\n",
    "    return (new_b, new_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize `b` and `m` as 0 and run a first iteration of the `step_gradient` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 0 \n",
    "m = 0 \n",
    "b, m = step_gradient(b,m,data) \n",
    "# b= 3.02503, m= 2.07286"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So just looking at input and output, we begin by setting $b$ and $m$ to 0 amnd 0.  Then from our step_gradient function, we receive new values of $b$ and $m$ of 3.02503 and 2.0728.  Now what we need to do, is take another step in the correct direction by calling our step gradient function with our updated values of $b$ and $m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b,m = step_gradient(b,m, data)\n",
    "# b = 5.63489, m= 3.902265"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this, say, 1000 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a for loop to do this\n",
    "iterations = [] \n",
    "for i in range(1000): \n",
    "    iteration = step_gradient(b,m, data)\n",
    "    b = iteration[0] \n",
    "    m = iteration[1] \n",
    "    iterations.append(iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the estimates in the last iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7.884345171519152, 5.5200596577756995),\n",
       " (9.820920294267818, 6.953902303201383),\n",
       " (11.485907009550733, 8.227813233586371),\n",
       " (12.915156619426247, 9.362652253147472),\n",
       " (14.139801542875658, 10.3765351615667),\n",
       " (15.186876343718733, 11.285194867591269),\n",
       " (16.0798570288238, 12.102294978739781),\n",
       " (16.839129372557856, 12.839702121442453),\n",
       " (17.482395607898194, 13.507722422849499),\n",
       " (18.025027595391315, 14.115306870770379),\n",
       " (18.48037351368167, 14.670229647501909),\n",
       " (18.86002418835148, 15.179242994285609),\n",
       " (19.174044370826213, 15.648211695053913),\n",
       " (19.431173580052743, 16.0822298616453),\n",
       " (19.639000512607243, 16.485722349683147),\n",
       " (19.804114499729806, 16.86253282778208),\n",
       " (19.932237031999197, 17.216000256555166),\n",
       " (20.028335974823563, 17.549025302736702),\n",
       " (20.0967247527028, 17.864128012997963),\n",
       " (20.14114848043059, 18.16349789771379),\n",
       " (20.164858759069432, 18.449037423559545),\n",
       " (20.17067862845862, 18.722399782361524),\n",
       " (20.161058971693627, 18.98502168946767),\n",
       " (20.1381274965307, 19.238151865772664),\n",
       " (20.103731270622212, 19.482875771444856),\n",
       " (20.05947365892371, 19.72013708464523),\n",
       " (20.00674639996872, 19.950756353609776),\n",
       " (19.946757460755236, 20.175447194091262),\n",
       " (19.880555225795167, 20.3948303552003),\n",
       " (19.80904950276514, 20.609445934172285),\n",
       " (19.733029763706508, 20.819763983668498),\n",
       " (19.65318098558706, 21.02619372315969),\n",
       " (19.570097406157934, 21.2290915381001),\n",
       " (19.484294469461116, 21.428767926423046),\n",
       " (19.39621919923661, 21.625493530894328),\n",
       " (19.306259207124032, 21.819504377627865),\n",
       " (19.21475051532502, 22.011006425235344),\n",
       " (19.121984349748228, 22.200179515332877),\n",
       " (19.028213039125525, 22.387180803188084),\n",
       " (18.933655137757196, 22.57214773692288),\n",
       " (18.838499874059735, 22.755200644683494),\n",
       " (18.742911013643383, 22.93644498137048),\n",
       " (18.6470302139697, 23.115973279731666),\n",
       " (18.55097993749939, 23.293866844724747),\n",
       " (18.454865981434878, 23.470197224935987),\n",
       " (18.358779674515485, 23.64502749039501),\n",
       " (18.26279978468241, 23.818413342264446),\n",
       " (18.16699417566444, 23.990404076530112),\n",
       " (18.071421245527443, 24.161043420905536),\n",
       " (17.97613117588229, 24.330370261636048),\n",
       " (17.88116701666936, 24.498419274691848),\n",
       " (17.786565628158584, 24.66522147393254),\n",
       " (17.692358498956175, 24.83080468716978),\n",
       " (17.598572456336118, 24.995193969616675),\n",
       " (17.505230283067128, 25.158411962963797),\n",
       " (17.41235125304074, 25.320479207237344),\n",
       " (17.319951596386733, 25.48141441165324),\n",
       " (17.228044903355837, 25.641234689863214),\n",
       " (17.13664247502827, 25.799955764278774),\n",
       " (17.045753627846192, 25.957592143542307),\n",
       " (16.955385958047177, 26.11415727667899),\n",
       " (16.86554557127601, 26.26966368699819),\n",
       " (16.776237281957613, 26.424123088409132),\n",
       " (16.687464786410814, 26.57754648646498),\n",
       " (16.599230813158865, 26.72994426614487),\n",
       " (16.51153725343787, 26.88132626811902),\n",
       " (16.424385274509266, 27.031701855012336),\n",
       " (16.337775418039595, 27.181079968982523),\n",
       " (16.25170768551281, 27.329469181755545),\n",
       " (16.16618161238192, 27.47687773811082),\n",
       " (16.081196332441966, 27.62331359367795),\n",
       " (15.99675063371144, 27.76878444779344),\n",
       " (15.91284300693973, 27.913297772067256),\n",
       " (15.82947168771122, 28.056860835223624),\n",
       " (15.74663469298885, 28.199480724706167),\n",
       " (15.664329852829061, 28.341164365473),\n",
       " (15.582554837903698, 28.481918536351365),\n",
       " (15.501307183380845, 28.621749884272774),\n",
       " (15.42058430964388, 28.760664936667354),\n",
       " (15.340383540264968, 28.89867011225948),\n",
       " (15.260702117594462, 29.03577173047486),\n",
       " (15.181537216280072, 29.17197601964156),\n",
       " (15.102885954988386, 29.307289124143562),\n",
       " (15.024745406565431, 29.44171711066445),\n",
       " (14.947112606841834, 29.575265973640736),\n",
       " (14.869984562261063, 29.707941640028707),\n",
       " (14.793358256485769, 29.83974997347486),\n",
       " (14.717230656116824, 29.970696777968264),\n",
       " (14.641598715641942, 30.10078780104281),\n",
       " (14.566459381715397, 30.230028736588366),\n",
       " (14.491809596856974, 30.358425227322165),\n",
       " (14.417646302646713, 30.485982866964864),\n",
       " (14.34396644248189, 30.61270720216002),\n",
       " (14.270766963953992, 30.738603734170518),\n",
       " (14.198044820895785, 30.863677920381097),\n",
       " (14.125796975142006, 30.987935175632355),\n",
       " (14.054020398041487, 31.11138087340816),\n",
       " (13.982712071753532, 31.234020346895605),\n",
       " (13.91186899035704, 31.355858889934094),\n",
       " (13.841488160797132, 31.47690175786794),\n",
       " (13.771566603690786, 31.597154168315004),\n",
       " (13.70210135401012, 31.716621301862247),\n",
       " (13.633089461659546, 31.8353083026976),\n",
       " (13.56452799196087, 31.9532202791864),\n",
       " (13.49641402605855, 32.070362304399445),\n",
       " (13.428744661255722, 32.18673941659892),\n",
       " (13.361517011290225, 32.3023566196875),\n",
       " (13.2947282065586, 32.417218883625345),\n",
       " (13.228375394295039, 32.53133114481901),\n",
       " (13.162455738711287, 32.64469830648581),\n",
       " (13.096966421102767, 32.75732523899665),\n",
       " (13.031904639925441, 32.869216780200084),\n",
       " (12.967267610847387, 32.98037773572977),\n",
       " (12.903052566778497, 33.090812879297424),\n",
       " (12.839256757881293, 33.200526952973),\n",
       " (12.775877451565423, 33.30952466745358),\n",
       " (12.712911932468103, 33.41781070232232),\n",
       " (12.650357502422429, 33.525389706298576),\n",
       " (12.588211480415275, 33.632266297480236),\n",
       " (12.526471202536223, 33.73844506357906),\n",
       " (12.465134021918814, 33.84393056214986),\n",
       " (12.40419730867521, 33.94872732081411),\n",
       " (12.343658449825243, 34.0528398374786),\n",
       " (12.283514849220676, 34.15627258054959),\n",
       " (12.22376392746539, 34.25902998914291),\n",
       " (12.164403121832144, 34.361116473290416),\n",
       " (12.105429886176431, 34.462536414143074),\n",
       " (12.04684169084791, 34.56329416417097),\n",
       " (11.988636022599826, 34.66339404736055),\n",
       " (11.930810384496773, 34.76284035940921),\n",
       " (11.873362295821096, 34.861637367917524),\n",
       " (11.816289291978206, 34.95978931257919),\n",
       " (11.759588924401049, 35.05730040536891),\n",
       " (11.703258760453902, 35.154174830728294),\n",
       " (11.647296383335698, 35.25041674574987),\n",
       " (11.591699391983012, 35.346030280359365),\n",
       " (11.53646540097284, 35.44101953749633),\n",
       " (11.481592040425289, 35.535388593293085),\n",
       " (11.42707695590626, 35.629141497252206),\n",
       " (11.372917808330232, 35.72228227242249),\n",
       " (11.319112273863192, 35.81481491557351),\n",
       " (11.26565804382579, 35.90674339736878),\n",
       " (11.212552824596758, 35.9980716625376),\n",
       " (11.159794337516665, 36.08880363004556),\n",
       " (11.107380318792014, 36.17894319326383),\n",
       " (11.055308519399732, 36.26849422013712),\n",
       " (11.003576704992085, 36.35746055335055),\n",
       " (10.952182655802025, 36.445846010495195),\n",
       " (10.90112416654902, 36.53365438423255),\n",
       " (10.85039904634534, 36.620889442457795),\n",
       " (10.800005118602863, 36.707554928461974),\n",
       " (10.749940220940369, 36.79365456109299),\n",
       " (10.700202205091363, 36.879192034915576),\n",
       " (10.650788936812422, 36.96417102037009),\n",
       " (10.601698295792065, 37.04859516393031),\n",
       " (10.552928175560181, 37.13246808826014),\n",
       " (10.504476483397971, 37.21579339236924),\n",
       " (10.45634114024846, 37.29857465176766),\n",
       " (10.408520080627541, 37.380815418619385),\n",
       " (10.36101125253557, 37.462519221894915),\n",
       " (10.313812617369512, 37.543689567522804),\n",
       " (10.266922149835638, 37.62432993854019),\n",
       " (10.220337837862761, 37.704443795242355),\n",
       " (10.17405768251603, 37.784034575331255),\n",
       " (10.128079697911271, 37.86310569406313),\n",
       " (10.082401911129862, 37.94166054439509),\n",
       " (10.037022362134163, 38.01970249713076),\n",
       " (9.991939103683483, 38.09723490106495),\n",
       " (9.947150201250592, 38.174261083127426),\n",
       " (9.902653732938754, 38.25078434852567),\n",
       " (9.858447789399316, 38.326807980886734),\n",
       " (9.814530473749816, 38.40233524239819),\n",
       " (9.770899901492623, 38.47736937394811),\n",
       " (9.727554200434104, 38.55191359526415),\n",
       " (9.684491510604317, 38.625971105051754),\n",
       " (9.641709984177224, 38.69954508113138),\n",
       " (9.599207785391414, 38.77263868057492),\n",
       " (9.556983090471357, 38.845255039841135),\n",
       " (9.515034087549155, 38.917397274910286),\n",
       " (9.473358976586812, 38.98906848141783),\n",
       " (9.431955969299, 39.06027173478727),\n",
       " (9.390823289076343, 39.131010090362125),\n",
       " (9.349959170909178, 39.20128658353703),\n",
       " (9.309361861311842, 39.27110422988802),\n",
       " (9.269029618247421, 39.34046602530188),\n",
       " (9.228960711053013, 39.40937494610477),\n",
       " (9.189153420365466, 39.477833949189886),\n",
       " (9.149606038047612, 39.54584597214436),\n",
       " (9.110316867114964, 39.61341393337534),\n",
       " (9.071284221662909, 39.68054073223519),\n",
       " (9.032506426794363, 39.747229249145896),\n",
       " (8.993981818547908, 39.81348234572271),\n",
       " (8.955708743826392, 39.87930286489691),\n",
       " (8.91768556032599, 39.94469363103779),\n",
       " (8.879910636465747, 40.009657450073874),\n",
       " (8.842382351317553, 40.07419710961331),\n",
       " (8.805099094536606, 40.138315379063485),\n",
       " (8.768059266292296, 40.20201500974987),\n",
       " (8.73126127719957, 40.26529873503407),\n",
       " (8.694703548250725, 40.32816927043112),\n",
       " (8.65838451074766, 40.39062931372599),\n",
       " (8.622302606234552, 40.45268154508939),\n",
       " (8.586456286430996, 40.51432862719275),\n",
       " (8.55084401316556, 40.57557320532245),\n",
       " (8.515464258309786, 40.63641790749338),\n",
       " (8.480315503712612, 40.69686534456169),\n",
       " (8.44539624113524, 40.75691811033679),\n",
       " (8.410704972186405, 40.81657878169272),\n",
       " (8.376240208258087, 40.87584991867865),\n",
       " (8.342000470461633, 40.93473406462877),\n",
       " (8.307984289564294, 40.99323374627144),\n",
       " (8.274190205926185, 41.05135147383758),\n",
       " (8.24061676943765, 41.10908974116839),\n",
       " (8.207262539457039, 41.16645102582238),\n",
       " (8.174126084748885, 41.22343778918167),\n",
       " (8.141205983422495, 41.280052476557614),\n",
       " (8.108500822870935, 41.336297517295705),\n",
       " (8.076009199710414, 41.39217532487986),\n",
       " (8.043729719720067, 41.44768829703595),\n",
       " (8.011660997782123, 41.50283881583471),\n",
       " (7.979801657822473, 41.55762924779391),\n",
       " (7.948150332751626, 41.612061943979974),\n",
       " (7.916705664406044, 41.666139240108805),\n",
       " (7.88546630348986, 41.71986345664603),\n",
       " (7.854430909516984, 41.77323689890658),\n",
       " (7.82359815075358, 41.826261857153604),\n",
       " (7.792966704160922, 41.87894060669674),\n",
       " (7.762535255338624, 41.93127540798975),\n",
       " (7.732302498468233, 41.98326850672753),\n",
       " (7.702267136257204, 42.034922133942466),\n",
       " (7.672427879883227, 42.08623850610015),\n",
       " (7.642783448938926, 42.13721982519452),\n",
       " (7.613332571376915, 42.18786827884232),\n",
       " (7.584073983455219, 42.238186040376995),\n",
       " (7.555006429683037, 42.28817526894193),\n",
       " (7.52612866276688, 42.337838109583096),\n",
       " (7.497439443557042, 42.387176693341125),\n",
       " (7.468937540994428, 42.43619313734273),\n",
       " (7.440621732057735, 42.48488954489154),\n",
       " (7.412490801710963, 42.53326800555837),\n",
       " (7.384543542851287, 42.5813305952709),\n",
       " (7.356778756257254, 42.629079376402714),\n",
       " (7.3291952505373255, 42.67651639786184),\n",
       " (7.301791842078755, 42.72364369517862),\n",
       " (7.274567354996798, 42.77046329059311),\n",
       " (7.247520621084258, 42.81697719314181),\n",
       " (7.220650479761354, 42.86318739874389),\n",
       " (7.193955778025923, 42.90909589028684),\n",
       " (7.167435370403942, 42.95470463771156),\n",
       " (7.14108811890038, 43.00001559809685),\n",
       " (7.11491289295036, 43.04503071574346),\n",
       " (7.088908569370651, 43.089751922257435),\n",
       " (7.06307403231147, 43.13418113663308),\n",
       " (7.037408173208604, 43.178320265335266),\n",
       " (7.0119098907358355, 43.22217120238122),\n",
       " (6.986578090757689, 43.265735829421835),\n",
       " (6.961411686282481, 43.30901601582239),\n",
       " (6.936409597415674, 43.35201361874276),\n",
       " (6.911570751313541, 43.39473048321714),\n",
       " (6.886894082137132, 43.43716844223317),\n",
       " (6.862378531006529, 43.47932931681063),\n",
       " (6.838023045955421, 43.521214916079536),\n",
       " (6.813826581885957, 43.56282703735782),\n",
       " (6.789788100523902, 43.60416746622841),\n",
       " (6.765906570374084, 43.64523797661589),\n",
       " (6.742180966676136, 43.686040330862575),\n",
       " (6.718610271360518, 43.72657627980417),\n",
       " (6.695193473004835, 43.76684756284488),\n",
       " (6.671929566790436, 43.806855908032055),\n",
       " (6.6488175544592965, 43.84660303213033),\n",
       " (6.625856444271183, 43.886090640695315),\n",
       " (6.603045250961099, 43.925320428146755),\n",
       " (6.580382995697004, 43.96429407784126),\n",
       " (6.557868706037815, 44.003013262144506),\n",
       " (6.535501415891676, 44.04147964250304),\n",
       " (6.513280165474504, 44.07969486951554),\n",
       " (6.491204001268807, 44.117660583003655),\n",
       " (6.469271975982764, 44.155378412082364),\n",
       " (6.44748314850958, 44.19284997522989),\n",
       " (6.425836583887099, 44.23007688035714),\n",
       " (6.404331353257691, 44.26706072487671),\n",
       " (6.3829665338283865, 44.303803095771414),\n",
       " (6.361741208831288, 44.34030556966242),\n",
       " (6.340654467484224, 44.376569712876844),\n",
       " (6.31970540495167, 44.41259708151502),\n",
       " (6.298893122305925, 44.448389221517225),\n",
       " (6.278216726488532, 44.48394766873005),\n",
       " (6.257675330271961, 44.51927394897229),\n",
       " (6.237268052221538, 44.55436957810041),\n",
       " (6.21699401665762, 44.58923606207361),\n",
       " (6.196852353618022, 44.62387489701841),\n",
       " (6.176842198820685, 44.6582875692929),\n",
       " (6.156962693626591, 44.69247555555048),\n",
       " (6.137212985002917, 44.726440322803235),\n",
       " (6.117592225486433, 44.76018332848489),\n",
       " (6.0980995731471355, 44.79370602051335),\n",
       " (6.078734191552121, 44.827009837352804),\n",
       " (6.059495249729695, 44.8600962080755),\n",
       " (6.040381922133714, 44.892966552423005),\n",
       " (6.021393388608162, 44.925622280867195),\n",
       " (6.002528834351956, 44.95806479467071),\n",
       " (5.983787449883985, 44.990295485947144),\n",
       " (5.9651684310083715, 45.022315737720724),\n",
       " (5.946670978779968, 45.0541269239857),\n",
       " (5.928294299470072, 45.08573040976528),\n",
       " (5.910037604532365, 45.11712755117022),\n",
       " (5.891900110569077, 45.14831969545699),\n",
       " (5.873881039297374, 45.17930818108562),\n",
       " (5.855979617515957, 45.210094337777086),\n",
       " (5.8381950770718865, 45.24067948657042),\n",
       " (5.82052665482762, 45.271064939879366),\n",
       " (5.802973592628267, 45.3012520015487),\n",
       " (5.785535137269054, 45.33124196691018),\n",
       " (5.7682105404630075, 45.36103612283812),\n",
       " (5.750999058808841, 45.390635747804644),\n",
       " (5.7338999537590585, 45.4200421119345),\n",
       " (5.716912491588264, 45.449256477059585),\n",
       " (5.700035943361675, 45.47828009677309),\n",
       " (5.683269584903842, 45.50711421648329),\n",
       " (5.66661269676758, 45.53576007346698),\n",
       " (5.650064564203092, 45.564218896922576),\n",
       " (5.6336244771273, 45.59249190802285),\n",
       " (5.617291730093375, 45.62058031996734),\n",
       " (5.601065622260469, 45.6484853380344),\n",
       " (5.584945457363639, 45.67620815963292),\n",
       " (5.568930543683968, 45.70374997435372),\n",
       " (5.553020194018889, 45.73111196402058),\n",
       " (5.537213725652693, 45.75829530274095),\n",
       " (5.521510460327234, 45.78530115695634),\n",
       " (5.505909724212827, 45.81213068549239),\n",
       " (5.490410847879331, 45.83878503960855),\n",
       " (5.475013166267424, 45.86526536304753),\n",
       " (5.459716018660068, 45.89157279208433),\n",
       " (5.444518748654155, 45.91770845557505),\n",
       " (5.42942070413234, 45.94367347500529),\n",
       " (5.414421237235061, 45.96946896453828),\n",
       " (5.399519704332737, 45.995096031062694),\n",
       " (5.384715465998152, 46.02055577424016),\n",
       " (5.370007886979016, 46.04584928655242),\n",
       " (5.355396336170704, 46.07097765334823),\n",
       " (5.34088018658918, 46.09594195288993),\n",
       " (5.326458815344088, 46.1207432563997),\n",
       " (5.312131603612026, 46.14538262810552),\n",
       " (5.297897936609995, 46.16986112528688),\n",
       " (5.283757203569012, 46.19417979832009),\n",
       " (5.2697087977079065, 46.218339690723404),\n",
       " (5.255752116207284, 46.24234183920175),\n",
       " (5.241886560183656, 46.26618727369125),\n",
       " (5.228111534663744, 46.289877017403406),\n",
       " (5.214426448558947, 46.313412086869),\n",
       " (5.20083071463998, 46.33679349198171),\n",
       " (5.187323749511674, 46.36002223604146),\n",
       " (5.173904973587945, 46.383099315797466),\n",
       " (5.16057381106692, 46.40602572149099),\n",
       " (5.1473296899062335, 46.42880243689786),\n",
       " (5.134172041798478, 46.45143043937066),\n",
       " (5.121100302146819, 46.47391069988068),\n",
       " (5.1081139100407675, 46.49624418305959),\n",
       " (5.095212308232112, 46.51843184724079),\n",
       " (5.082394943111007, 46.5404746445006),\n",
       " (5.069661264682217, 46.56237352069905),\n",
       " (5.057010726541515, 46.58412941552051),\n",
       " (5.0444427858522385, 46.60574326251398),\n",
       " (5.031956903321994, 46.627215989133184),\n",
       " (5.019552543179519, 46.64854851677632),\n",
       " (5.007229173151689, 46.66974176082566),\n",
       " (4.994986264440679, 46.69079663068676),\n",
       " (4.982823291701271, 46.71171402982755),\n",
       " (4.970739733018313, 46.73249485581704),\n",
       " (4.958735069884324, 46.75314000036388),\n",
       " (4.946808787177243, 46.7736503493546),\n",
       " (4.934960373138328, 46.79402678289163),\n",
       " (4.9231893193501906, 46.814270175331075),\n",
       " (4.911495120714987, 46.83438139532022),\n",
       " (4.899877275432738, 46.85436130583481),\n",
       " (4.888335284979802, 46.87421076421606),\n",
       " (4.876868654087478, 46.89393062220749),\n",
       " (4.865476890720758, 46.913521725991444),\n",
       " (4.854159506057209, 46.93298491622538),\n",
       " (4.842916014466003, 46.95232102807798),\n",
       " (4.831745933487076, 46.971530891264976),\n",
       " (4.820648783810423, 46.99061533008473),\n",
       " (4.809624089255536, 47.00957516345365),\n",
       " (4.798671376750964, 47.02841120494128),\n",
       " (4.787790176314023, 47.04712426280525),\n",
       " (4.77698002103062, 47.06571514002593),\n",
       " (4.766240447035221, 47.08418463434091),\n",
       " (4.755570993490949, 47.10253353827921),\n",
       " (4.744971202569804, 47.12076263919531),\n",
       " (4.734440619433023, 47.13887271930292),\n",
       " (4.72397879221156, 47.15686455570852),\n",
       " (4.7135852719866955, 47.17473892044478),\n",
       " (4.7032596127707755, 47.1924965805036),\n",
       " (4.693001371488072, 47.21013829786909),\n",
       " (4.682810107955772, 47.22766482955023),\n",
       " (4.6726853848650896, 47.24507692761336),\n",
       " (4.662626767762498, 47.262375339214465),\n",
       " (4.652633825031091, 47.27956080663121),\n",
       " (4.642706127872061, 47.29663406729482),\n",
       " (4.632843250286296, 47.3135958538217),\n",
       " (4.623044769056104, 47.3304468940449),\n",
       " (4.613310263727053, 47.3471879110453),\n",
       " (4.603639316589923, 47.36381962318269),\n",
       " (4.5940315126627915, 47.380342744126565),\n",
       " (4.584486439673217, 47.39675798288674),\n",
       " (4.575003688040556, 47.4130660438438),\n",
       " (4.565582850858383, 47.42926762677931),\n",
       " (4.5562235238770326, 47.44536342690584),\n",
       " (4.546925305486251, 47.46135413489683),\n",
       " (4.537687796697965, 47.47724043691616),\n",
       " (4.5285106011291605, 47.493023014647655),\n",
       " (4.519393324984872, 47.508702545324304),\n",
       " (4.510335577041288, 47.524279701757344),\n",
       " (4.501336968628962, 47.5397551523651),\n",
       " (4.492397113616133, 47.5551295612017),\n",
       " (4.483515628392159, 47.570403587985545),\n",
       " (4.4746921318510555, 47.58557788812762),\n",
       " (4.46592624537514, 47.60065311275962),\n",
       " (4.45721759281879, 47.61562990876192),\n",
       " (4.448565800492297, 47.63050891879127),\n",
       " (4.439970497145837, 47.64529078130845),\n",
       " (4.431431313953536, 47.65997613060558),\n",
       " (4.422947884497644, 47.6745655968334),\n",
       " (4.414519844752816, 47.68905980602832),\n",
       " (4.406146833070484, 47.7034593801392),\n",
       " (4.397828490163349, 47.71776493705414),\n",
       " (4.389564459089953, 47.73197709062693),\n",
       " (4.38135438523937, 47.7460964507034),\n",
       " (4.373197916315988, 47.7601236231476),\n",
       " (4.365094702324388, 47.7740592098678),\n",
       " (4.3570443955543325, 47.787903808842316),\n",
       " (4.34904665056584, 47.80165801414515),\n",
       " (4.3411011241743624, 47.815322415971494),\n",
       " (4.333207475436063, 47.828897600663076),\n",
       " (4.325365365633181, 47.842384150733295),\n",
       " (4.317574458259501, 47.85578264489222),\n",
       " (4.309834419005912, 47.869093658071435),\n",
       " (4.302144915746063, 47.88231776144869),\n",
       " (4.29450561852211, 47.895455522472425),\n",
       " (4.286916199530557, 47.90850750488612),\n",
       " (4.279376333108194, 47.92147426875247),\n",
       " (4.271885695718119, 47.93435637047744),\n",
       " (4.264443965935853, 47.94715436283413),\n",
       " (4.257050824435554, 47.95986879498648),\n",
       " (4.24970595397631, 47.972500212512855),\n",
       " (4.2424090393885265, 47.98504915742945),\n",
       " (4.235159767560404, 47.99751616821355),\n",
       " (4.227957827424501, 48.00990177982663),\n",
       " (4.220802909944388, 48.0222065237373),\n",
       " (4.213694708101384, 48.03443092794415),\n",
       " (4.206632916881384, 48.04657551699836),\n",
       " (4.199617233261772, 48.05864081202623),\n",
       " (4.192647356198413, 48.070627330751556),\n",
       " (4.185722986612743, 48.0825355875178),\n",
       " (4.178843827378928, 48.09436609331023),\n",
       " (4.17200958331112, 48.10611935577777),\n",
       " (4.165219961150788, 48.11779587925486),\n",
       " (4.158474669554131, 48.12939616478301),\n",
       " (4.151773419079584, 48.140920710132384),\n",
       " (4.145115922175392, 48.1523700098231),\n",
       " (4.1385018931672715, 48.163744555146494),\n",
       " (4.131931048246156, 48.17504483418616),\n",
       " (4.125403105456014, 48.186271331838924),\n",
       " (4.118917784681752, 48.19742452983564),\n",
       " (4.112474807637193, 48.20850490676185),\n",
       " (4.106073897853138, 48.21951293807836),\n",
       " (4.0997147806655, 48.230449096141584),\n",
       " (4.0933971832035185, 48.24131385022386),\n",
       " (4.0871208343780525, 48.25210766653356),\n",
       " (4.080885464869946, 48.26283100823512),\n",
       " (4.074690807118474, 48.27348433546887),\n",
       " (4.0685365953098565, 48.28406810537084),\n",
       " (4.062422565365857, 48.294582772092326),\n",
       " (4.056348454932451, 48.30502878681939),\n",
       " (4.050314003368564, 48.31540659779225),\n",
       " (4.044318951734893, 48.32571665032446),\n",
       " (4.03836304278279, 48.33595938682206),\n",
       " (4.032446020943227, 48.34613524680256),\n",
       " (4.026567632315827, 48.35624466691378),\n",
       " (4.020727624657973, 48.366288080952586),\n",
       " (4.01492574737398, 48.37626591988352),\n",
       " (4.009161751504344, 48.38617861185728),\n",
       " (4.003435389715057, 48.39602658222911),\n",
       " (3.997746416286999, 48.40581025357703),\n",
       " (3.9920945871053872, 48.41553004571997),\n",
       " (3.9864796596493077, 48.425186375735805),\n",
       " (3.9809013929813033, 48.43477965797924),\n",
       " (3.975359547737038, 48.444310304099595),\n",
       " (3.9698538861150245, 48.45377872305844),\n",
       " (3.964384171866422, 48.46318532114719),\n",
       " (3.958950170284896, 48.47253050200452),\n",
       " (3.9535516481965494, 48.481814666633646),\n",
       " (3.948188373949915, 48.491038213419614),\n",
       " (3.9428601174060174, 48.500201538146314),\n",
       " (3.937566649928496, 48.509305034013515),\n",
       " (3.932307744373794, 48.51834909165372),\n",
       " (3.9270831750814135, 48.52733409914891),\n",
       " (3.921892717864231, 48.53626044204724),\n",
       " (3.916736149998877, 48.54512850337954),\n",
       " (3.9116132502161802, 48.55393866367577),\n",
       " (3.9065237986916728, 48.56269130098136),\n",
       " (3.9014675770361555, 48.57138679087341),\n",
       " (3.89644436828633, 48.58002550647681),\n",
       " (3.8914539568954862, 48.588607818480256),\n",
       " (3.886496128724254, 48.597134095152164),\n",
       " (3.8815706710314157, 48.605604702356466),\n",
       " (3.876677372464775, 48.6140200035683),\n",
       " (3.871816023052089, 48.62238035988962),\n",
       " (3.8669864141920587, 48.63068613006469),\n",
       " (3.8621883386453773, 48.63893767049546),\n",
       " (3.857421590525837, 48.647135335256884),\n",
       " (3.852685965291496, 48.6552794761121),\n",
       " (3.8479812597359, 48.66337044252752),\n",
       " (3.8433072719793637, 48.67140858168785),\n",
       " (3.838663801460307, 48.679394238510945),\n",
       " (3.8340506489266497, 48.68732775566265),\n",
       " (3.8294676164272614, 48.69520947357149),\n",
       " (3.824914507303467, 48.703039730443265),\n",
       " (3.8203911261806085, 48.71081886227558),\n",
       " (3.8158972789596617, 48.71854720287226),\n",
       " (3.811432772808906, 48.726225083857656),\n",
       " (3.8069974161556526, 48.73385283469091),\n",
       " (3.8025910186780205, 48.74143078268005),\n",
       " (3.7982133912967724, 48.748959252996066),\n",
       " (3.7938643461672013, 48.75643856868686),\n",
       " (3.789543696671068, 48.763869050691085),\n",
       " (3.7852512574085955, 48.771251017851945),\n",
       " (3.780986844190512, 48.77858478693086),\n",
       " (3.7767502740301486, 48.78587067262105),\n",
       " (3.7725413651355857, 48.79310898756108),\n",
       " (3.7683599369018537, 48.80030004234821),\n",
       " (3.7642058099031828, 48.8074441455518),\n",
       " (3.760078805885303, 48.81454160372647),\n",
       " (3.7559787477577964, 48.82159272142532),\n",
       " (3.751905459586497, 48.828597801212965),\n",
       " (3.747858766585943, 48.83555714367853),\n",
       " (3.743838495111875, 48.84247104744853),\n",
       " (3.7398444726537856, 48.84933980919971),\n",
       " (3.7358765278275174, 48.85616372367177),\n",
       " (3.731934490367909, 48.86294308367999),\n",
       " (3.7280181911214862, 48.86967818012781),\n",
       " (3.724127462039208, 48.87636930201934),\n",
       " (3.7202621361692523, 48.8830167364717),\n",
       " (3.716422047649853, 48.8896207687274),\n",
       " (3.7126070317021838, 48.896181682166535),\n",
       " (3.708816924623285, 48.90269975831899),\n",
       " (3.7050515637790418, 48.90917527687647),\n",
       " (3.7013107875972033, 48.915608515704534),\n",
       " (3.6975944355604518, 48.92199975085452),\n",
       " (3.6939023481995124, 48.928349256575366),\n",
       " (3.6902343670863123, 48.93465730532541),\n",
       " (3.686590334827182, 48.940924167784054),\n",
       " (3.6829700950560995, 48.9471501128634),\n",
       " (3.679373492427984, 48.953335407719756),\n",
       " (3.675800372612028, 48.95948031776515),\n",
       " (3.672250582285075, 48.96558510667867),\n",
       " (3.6687239691250406, 48.971650036417806),\n",
       " (3.6652203818043776, 48.97767536722969),\n",
       " (3.66173966998358, 48.983661357662264),\n",
       " (3.658281684304733, 48.98960826457534),\n",
       " (3.654846276385106, 48.99551634315167),\n",
       " (3.6514332988107823, 49.00138584690789),\n",
       " (3.6480426051303354, 49.007217027705344),\n",
       " (3.6446740498485446, 49.01301013576097),\n",
       " (3.6413274884201505, 49.01876541965798),\n",
       " (3.638002777243655, 49.02448312635655),\n",
       " (3.6346997736551567, 49.030163501204406),\n",
       " (3.631418335922231, 49.035806787947365),\n",
       " (3.6281583232378476, 49.04141322873978),\n",
       " (3.6249195957143288, 49.046983064154944),\n",
       " (3.621702014377346, 49.052516533195416),\n",
       " (3.6185054411599573, 49.05801387330325),\n",
       " (3.6153297388966825, 49.063475320370216),\n",
       " (3.6121747713176178, 49.068901108747895),\n",
       " (3.609040403042588, 49.07429147125776),\n",
       " (3.605926499575337, 49.079646639201144),\n",
       " (3.6028329272977566, 49.084966842369184),\n",
       " (3.599759553464155, 49.09025230905267),\n",
       " (3.596706246195557, 49.09550326605184),\n",
       " (3.593672874474048, 49.100719938686126),\n",
       " (3.590659308137151, 49.1059025508038),\n",
       " (3.5876654178722407, 49.11105132479159),\n",
       " (3.5846910752109955, 49.11616648158423),\n",
       " (3.5817361525238853, 49.12124824067394),\n",
       " (3.578800523014694, 49.12629682011984),\n",
       " (3.5758840607150786, 49.131312436557295),\n",
       " (3.5729866404791646, 49.136295305207234),\n",
       " (3.5701081379781745, 49.14124563988536),\n",
       " (3.567248429695095, 49.14616365301135),\n",
       " (3.564407392919375, 49.151049555617945),\n",
       " (3.5615849057416615, 49.15590355736003),\n",
       " (3.5587808470485682, 49.160725866523606),\n",
       " (3.5559950965174774, 49.16551669003476),\n",
       " (3.5532275346113775, 49.170276233468506),\n",
       " (3.5504780425737343, 49.17500470105764),\n",
       " (3.547746502423394, 49.179702295701475),\n",
       " (3.545032796949522, 49.18436921897457),\n",
       " (3.5423368097065713, 49.189005671135355),\n",
       " (3.539658425009289, 49.19361185113477),\n",
       " (3.536997527927748, 49.198187956624736),\n",
       " (3.5343540042824197, 49.2027341839667),\n",
       " (3.5317277406392713, 49.20725072824001),\n",
       " (3.5291186243049, 49.211737783250335),\n",
       " (3.526526543321697, 49.21619554153793),\n",
       " (3.5239513864630427, 49.220624194385934),\n",
       " (3.5213930432285347, 49.225023931828574),\n",
       " (3.5188514038392458, 49.22939494265929),\n",
       " (3.5163263592330143, 49.233737414438885),\n",
       " (3.513817801059762, 49.23805153350353),\n",
       " (3.511325621676847, 49.24233748497278),\n",
       " (3.508849714144444, 49.24659545275751),\n",
       " (3.5063899722209557, 49.25082561956782),\n",
       " (3.503946290358453, 49.25502816692086),\n",
       " (3.501518563698147, 49.25920327514862),\n",
       " (3.4991066880658903, 49.263351123405684),\n",
       " (3.4967105599677035, 49.26747188967689),\n",
       " (3.4943300765853387, 49.27156575078501),\n",
       " (3.4919651357718635, 49.27563288239828),\n",
       " (3.4896156360472808, 49.27967345903799),\n",
       " (3.4872814765941724, 49.28368765408594),\n",
       " (3.484962557253374, 49.2876756397919),\n",
       " (3.4826587785196774, 49.29163758728099),\n",
       " (3.48037004153756, 49.295573666561026),\n",
       " (3.4780962480969424, 49.29948404652981),\n",
       " (3.4758373006289767, 49.3033688949824),\n",
       " (3.4735931022018565, 49.30722837861826),\n",
       " (3.47136355651666, 49.31106266304848),\n",
       " (3.469148567903217, 49.31487191280282),\n",
       " (3.4669480413160034, 49.31865629133682),\n",
       " (3.4647618823300634, 49.322415961038786),\n",
       " (3.4625899971369565, 49.326151083236745),\n",
       " (3.460432292540734, 49.329861818205416),\n",
       " (3.4582886759539373, 49.33354832517304),\n",
       " (3.4561590553936288, 49.33721076232823),\n",
       " (3.4540433394774412, 49.340849286826774),\n",
       " (3.451941437419658, 49.344464054798344),\n",
       " (3.4498532590273183, 49.34805522135323),\n",
       " (3.447778714696345, 49.351622940588975),\n",
       " (3.4457177154077008, 49.355167365597),\n",
       " (3.4436701727235692, 49.358688648469155),\n",
       " (3.4416359987835587, 49.36218694030427),\n",
       " (3.4396151063009324, 49.36566239121461),\n",
       " (3.4376074085588626, 49.36911515033236),\n",
       " (3.4356128194067113, 49.372545365815974),\n",
       " (3.4336312532563302, 49.37595318485655),\n",
       " (3.4316626250783924, 49.379338753684166),\n",
       " (3.4297068503987402, 49.38270221757414),\n",
       " (3.427763845294762, 49.38604372085325),\n",
       " (3.42583352639179, 49.38936340690596),\n",
       " (3.423915810859525, 49.39266141818053),\n",
       " (3.4220106164084787, 49.39593789619517),\n",
       " (3.420117861286446, 49.39919298154409),\n",
       " (3.418237464274994, 49.40242681390353),\n",
       " (3.4163693446859793, 49.40563953203777),\n",
       " (3.414513422358085, 49.40883127380506),\n",
       " (3.4126696176533797, 49.41200217616358),\n",
       " (3.410837851453902, 49.415152375177264),\n",
       " (3.4090180451582635, 49.418282006021656),\n",
       " (3.407210120678279, 49.42139120298974),\n",
       " (3.405414000435613, 49.42448009949766),\n",
       " (3.403629607358451, 49.42754882809047),\n",
       " (3.401856864878195, 49.430597520447826),\n",
       " (3.400095696926174, 49.43362630738961),\n",
       " (3.3983460279303834, 49.436635318881564),\n",
       " (3.3966077828122403, 49.439624684040865),\n",
       " (3.3948808869833615, 49.442594531141665),\n",
       " (3.393165266342365, 49.445544987620586),\n",
       " (3.391460847271687, 49.448476180082196),\n",
       " (3.389767556634427, 49.451388234304446),\n",
       " (3.388085321771207, 49.454281275244064),\n",
       " (3.3864140704970525, 49.457155427041904),\n",
       " (3.3847537310982982, 49.46001081302829),\n",
       " (3.383104232329509, 49.4628475557283),\n",
       " (3.381465503410422, 49.46566577686704),\n",
       " (3.3798374740229113, 49.46846559737483),\n",
       " (3.37822007430797, 49.47124713739243),\n",
       " (3.37661323486271, 49.474010516276174),\n",
       " (3.3750168867373893, 49.476755852603105),\n",
       " (3.3734309614324474, 49.479483264176054),\n",
       " (3.37185539089557, 49.4821928680287),\n",
       " (3.3702901075187683, 49.48488478043058),\n",
       " (3.3687350441354753, 49.487559116892115),\n",
       " (3.3671901340176666, 49.4902159921695),\n",
       " (3.365655310872996, 49.4928555202697),\n",
       " (3.3641305088419515, 49.49547781445529),\n",
       " (3.362615662495028, 49.49808298724935),\n",
       " (3.3611107068299213, 49.50067115044026),\n",
       " (3.359615577268737, 49.50324241508654),\n",
       " (3.3581302096552212, 49.50579689152157),\n",
       " (3.356654540252006, 49.50833468935835),\n",
       " (3.3551885057378756, 49.510855917494204),\n",
       " (3.3537320432050493, 49.51336068411545),\n",
       " (3.3522850901564816, 49.51584909670202),\n",
       " (3.3508475845031795, 49.51832126203212),\n",
       " (3.349419464561541, 49.52077728618675),\n",
       " (3.348000669050705, 49.52321727455433),\n",
       " (3.346591137089924, 49.525641331835146),\n",
       " (3.3451908081959507, 49.5280495620459),\n",
       " (3.343799622280443, 49.53044206852414),\n",
       " (3.3424175196473858, 49.53281895393271),\n",
       " (3.3410444409905287, 49.53518032026416),\n",
       " (3.3396803273908415, 49.5375262688451),\n",
       " (3.3383251203139865, 49.53985690034057),\n",
       " (3.3369787616078064, 49.54217231475834),\n",
       " (3.3356411934998285, 49.54447261145324),\n",
       " (3.334312358594786, 49.54675788913136),\n",
       " (3.332992199872156, 49.549028245854345),\n",
       " (3.3316806606837117, 49.55128377904358),\n",
       " (3.3303776847510913, 49.55352458548436),\n",
       " (3.329083216163384, 49.55575076133005),\n",
       " (3.327797199374731, 49.55796240210623),\n",
       " (3.3265195792019413, 49.560159602714755),\n",
       " (3.3252503008221246, 49.56234245743788),\n",
       " (3.323989309770337, 49.56451105994224),\n",
       " (3.322736551937247, 49.56666550328294),\n",
       " (3.32149197356681, 49.56880587990749),\n",
       " (3.3202555212539644, 49.57093228165979),\n",
       " (3.31902714194234, 49.57304479978411),\n",
       " (3.3178067829219784, 49.575143524928926),\n",
       " (3.3165943918270746, 49.577228547150895),\n",
       " (3.315389916633728, 49.57929995591865),\n",
       " (3.3141933056577115, 49.58135784011669),\n",
       " (3.3130045075522516, 49.58340228804915),\n",
       " (3.3118234713058277, 49.585433387443636),\n",
       " (3.31065014623998, 49.587451225454956),\n",
       " (3.3094844820071385, 49.589455888668866),\n",
       " (3.3083264285884604, 49.591447463105794),\n",
       " (3.3071759362916833, 49.59342603422453),\n",
       " (3.3060329557489943, 49.59539168692587),\n",
       " (3.3048974379149114, 49.59734450555631),\n",
       " (3.3037693340641785, 49.5992845739116),\n",
       " (3.3026485957896745, 49.60121197524041),\n",
       " (3.3015351750003368, 49.603126792247835),\n",
       " (3.3004290239190963, 49.605029107098986),\n",
       " (3.2993300950808298, 49.606919001422504),\n",
       " (3.298238341330321, 49.60879655631407),\n",
       " (3.2971537158202375, 49.61066185233987),\n",
       " (3.296076172009121, 49.61251496954005),\n",
       " (3.2950056636593903, 49.61435598743219),\n",
       " (3.293942144835357, 49.61618498501466),\n",
       " (3.292885569901254, 49.61800204077005),\n",
       " (3.2918358935192775, 49.61980723266852),\n",
       " (3.2907930706476427, 49.621600638171145),\n",
       " (3.289757056538649, 49.62338233423325),\n",
       " (3.288727806736762, 49.6251523973077),\n",
       " (3.287705277076704, 49.626910903348204),\n",
       " (3.28668942368156, 49.62865792781254),\n",
       " (3.2856802029608936, 49.63039354566581),\n",
       " (3.2846775716088787, 49.63211783138366),\n",
       " (3.2836814866024406, 49.63383085895547),\n",
       " (3.282691905199408, 49.63553270188754),\n",
       " (3.2817087849366815, 49.63722343320623),\n",
       " (3.2807320836284086, 49.63890312546109),\n",
       " (3.279761759364177, 49.640571850727994),\n",
       " (3.2787977705072135, 49.64222968061222),\n",
       " (3.2778400756925987, 49.64387668625151),\n",
       " (3.276888633825491, 49.64551293831915),\n",
       " (3.2759434040793636, 49.64713850702699),\n",
       " (3.2750043458942546, 49.64875346212844),\n",
       " (3.274071418975023, 49.6503578729215),\n",
       " (3.2731445832896213, 49.65195180825169),\n",
       " (3.272223799067379, 49.653535336515034),\n",
       " (3.271309026797294, 49.655108525661),\n",
       " (3.270400227226339, 49.656671443195386),\n",
       " (3.269497361357775, 49.65822415618324),\n",
       " (3.2686003904494805, 49.65976673125172),\n",
       " (3.2677092760122863, 49.66129923459299),\n",
       " (3.2668239798083265, 49.66282173196701),\n",
       " (3.265944463849396, 49.664334288704396),\n",
       " (3.2650706903953206, 49.665836969709204),\n",
       " (3.264202621952339, 49.66732983946172),\n",
       " (3.2633402212714917, 49.668812962021235),\n",
       " (3.262483451347024, 49.670286401028775),\n",
       " (3.261632275414799, 49.67175021970986),\n",
       " (3.2607866569507173, 49.67320448087717),\n",
       " (3.2599465596691526, 49.6746492469333),\n",
       " (3.259111947521393, 49.676084579873404),\n",
       " (3.2582827846940945, 49.677510541287845),\n",
       " (3.257459035607744, 49.67892719236487),\n",
       " (3.256640664915134, 49.68033459389319),\n",
       " (3.255827637499844, 49.681732806264655),\n",
       " (3.2550199184747344, 49.68312188947678),\n",
       " (3.254217473180449, 49.68450190313535),\n",
       " (3.2534202671839303, 49.68587290645698),\n",
       " (3.252628266276937, 49.687234958271645),\n",
       " (3.251841436474581, 49.68858811702521),\n",
       " (3.251059744013867, 49.689932440781945),\n",
       " (3.2502831553522435, 49.691267987227),\n",
       " (3.2495116371661634, 49.69259481366889),\n",
       " (3.248745156349655, 49.693912977041954),\n",
       " (3.247983680012901, 49.6952225339088),\n",
       " (3.2472271754808277, 49.69652354046272),\n",
       " (3.2464756102917014, 49.697816052530136),\n",
       " (3.2457289521957366, 49.69910012557294),\n",
       " (3.2449871691537124, 49.700375814690915),\n",
       " (3.244250229335597, 49.701643174624095),\n",
       " (3.2435181011191814, 49.7029022597551),\n",
       " (3.242790753088725, 49.70415312411148),\n",
       " (3.2420681540336043, 49.70539582136802),\n",
       " (3.241350272946976, 49.70663040484908),\n",
       " (3.2406370790244448, 49.70785692753083),\n",
       " (3.2399285416627426, 49.709075442043556),\n",
       " (3.2392246304584154, 49.71028600067392),\n",
       " (3.2385253152065183, 49.71148865536719),\n",
       " (3.237830565899319, 49.712683457729476),\n",
       " (3.237140352725011, 49.71387045902994),\n",
       " (3.2364546460664343, 49.71504971020301),\n",
       " (3.2357734164998035, 49.71622126185054),\n",
       " (3.2350966347934467, 49.71738516424401),\n",
       " (3.23442427190655, 49.71854146732665),\n",
       " (3.233756298987912, 49.71969022071563),\n",
       " (3.233092687374705, 49.72083147370413),\n",
       " (3.2324334085912456, 49.721965275263514),\n",
       " (3.231778434347774, 49.723091674045385),\n",
       " (3.2311277365392383, 49.7242107183837),\n",
       " (3.230481287244089, 49.725322456296844),\n",
       " (3.229839058723081, 49.72642693548966),\n",
       " (3.2292010234180832, 49.72752420335554),\n",
       " (3.228567153950896, 49.72861430697843),\n",
       " (3.227937423122077, 49.72969729313485),\n",
       " (3.2273118039097732, 49.73077320829591),\n",
       " (3.2266902694685617, 49.73184209862931),\n",
       " (3.226072793128298, 49.732904010001306),\n",
       " (3.225459348392971, 49.733958987978696),\n",
       " (3.224849908939567, 49.73500707783075),\n",
       " (3.224244448616939, 49.73604832453118),\n",
       " (3.223642941444685, 49.737082772760054),\n",
       " (3.2230453616120345, 49.73811046690571),\n",
       " (3.2224516834767387, 49.73913145106667),\n",
       " (3.2218618815639717, 49.740145769053534),\n",
       " (3.2212759305652368, 49.741153464390855),\n",
       " (3.220693805337281, 49.742154580319),\n",
       " (3.2201154809010153, 49.743149159796026),\n",
       " (3.2195409324404434, 49.744137245499495),\n",
       " (3.218970135301597, 49.74511887982833),\n",
       " (3.218403064991477, 49.74609410490462),\n",
       " (3.217839697177004, 49.747062962575434),\n",
       " (3.217280007683972, 49.748025494414605),\n",
       " (3.2167239724960135, 49.748981741724535),\n",
       " (3.2161715677535683, 49.74993174553796),\n",
       " (3.215622769752858, 49.75087554661969),\n",
       " (3.215077554944871, 49.751813185468386),\n",
       " (3.21453589993435, 49.75274470231829),\n",
       " (3.21399778147879, 49.753670137140936),\n",
       " (3.21346317648744, 49.75458952964689),\n",
       " (3.2129320620203123, 49.75550291928744),\n",
       " (3.212404415287198, 49.75641034525626),\n",
       " (3.211880213646689, 49.75731184649118),\n",
       " (3.211359434605208, 49.75820746167574),\n",
       " (3.2108420558160407, 49.75909722924094),\n",
       " (3.2103280550783784, 49.75998118736686),\n",
       " (3.2098174103363655, 49.76085937398428),\n",
       " (3.2093100996781527, 49.76173182677634),\n",
       " (3.208806101334956, 49.76259858318013),\n",
       " (3.2083053936801242, 49.7634596803883),\n",
       " (3.2078079552282097, 49.76431515535068),\n",
       " (3.2073137646340477, 49.765165044775834),\n",
       " (3.2068228006918384, 49.76600938513267),\n",
       " (3.206335042334239, 49.76684821265196),\n",
       " (3.205850468631458, 49.767681563327926),\n",
       " (3.205369058790359, 49.768509472919796),\n",
       " (3.2048907921535665, 49.769331976953296),\n",
       " (3.204415648198581, 49.77014911072221),\n",
       " (3.203943606536898, 49.77096090928988),\n",
       " (3.203474646913132, 49.77176740749073),\n",
       " (3.2030087492041486, 49.772568639931706),\n",
       " (3.2025458934182014, 49.77336464099384),\n",
       " (3.2020860596940723, 49.77415544483367),\n",
       " (3.2016292283002215, 49.774941085384704),\n",
       " (3.2011753796339386, 49.77572159635891),\n",
       " (3.2007244942205033, 49.77649701124815),\n",
       " (3.200276552712349, 49.777267363325606),\n",
       " (3.1998315358882317, 49.778032685647204),\n",
       " (3.199389424652406, 49.77879301105305),\n",
       " (3.1989502000338077, 49.77954837216883),\n",
       " (3.198513843185236, 49.78029880140722),\n",
       " (3.198080335382547, 49.78104433096926),\n",
       " (3.19764965802385, 49.78178499284574),\n",
       " (3.1972217926287088, 49.78252081881859),\n",
       " (3.1967967208373493, 49.78325184046221),\n",
       " (3.1963744244098717, 49.78397808914488),\n",
       " (3.1959548852254667, 49.78469959603003),\n",
       " (3.1955380852816395, 49.78541639207765),\n",
       " (3.1951240066934363, 49.78612850804557),\n",
       " (3.194712631692677, 49.786835974490806),\n",
       " (3.1943039426271924, 49.78753882177085),\n",
       " (3.193897921960069, 49.78823708004501),\n",
       " (3.1934945522688927, 49.788930779275645),\n",
       " (3.193093816245004, 49.78961994922952),\n",
       " (3.192695696692755, 49.790304619479016),\n",
       " (3.19230017652877, 49.790984819403455),\n",
       " (3.1919072387812144, 49.791660578190324),\n",
       " (3.191516866589065, 49.792331924836546),\n",
       " (3.191129043201387, 49.79299888814971),\n",
       " (3.190743751976615, 49.79366149674932),\n",
       " (3.1903609763818404, 49.79431977906803),\n",
       " (3.1899806999920988, 49.794973763352836),\n",
       " (3.189602906489668, 49.795623477666304),\n",
       " (3.189227579663368, 49.79626894988779),\n",
       " (3.188854703407862, 49.79691020771461),\n",
       " (3.1884842617229694, 49.797547278663245),\n",
       " (3.188116238712977, 49.79818019007051),\n",
       " (3.187750618585957, 49.798808969094736),\n",
       " (3.18738738565309, 49.799433642716934),\n",
       " (3.1870265243279916, 49.80005423774195),\n",
       " (3.186668019126043, 49.80067078079963),\n",
       " (3.1863118546637277, 49.80128329834592),\n",
       " (3.18595801565797, 49.80189181666407),\n",
       " (3.1856064869254803, 49.80249636186568),\n",
       " (3.185257253382103, 49.803096959891896),\n",
       " (3.184910300042169, 49.803693636514474),\n",
       " (3.1845656120178543, 49.804286417336904),\n",
       " (3.184223174518539, 49.80487532779551),\n",
       " (3.1838829728501747, 49.80546039316053),\n",
       " (3.1835449924146513, 49.80604163853722),\n",
       " (3.183209218709173, 49.80661908886692),\n",
       " (3.1828756373256364, 49.807192768928104),\n",
       " (3.1825442339500083, 49.80776270333749),\n",
       " (3.182214994361716, 49.80832891655105),\n",
       " (3.1818879044330357, 49.80889143286509),\n",
       " (3.181562950128484, 49.80945027641727),\n",
       " (3.1812401175042186, 49.81000547118767),\n",
       " (3.1809193927074393, 49.81055704099978),\n",
       " (3.180600761975792, 49.811105009521555),\n",
       " (3.18028421163678, 49.81164940026641),\n",
       " (3.179969728107176, 49.81219023659425),\n",
       " (3.1796572978924407, 49.812727541712434),\n",
       " (3.1793469075861425, 49.813261338676824),\n",
       " (3.1790385438693822, 49.81379165039274),\n",
       " (3.1787321935102226, 49.81431849961593),\n",
       " (3.1784278433631195, 49.81484190895359),\n",
       " (3.178125480368358, 49.8153619008653),\n",
       " (3.177825091551493, 49.815878497663995),\n",
       " (3.177526664022791, 49.81639172151693),\n",
       " (3.1772301849766773, 49.81690159444663),\n",
       " (3.176935641691188, 49.817408138331814),\n",
       " (3.1766430215274224, 49.81791137490837),\n",
       " (3.1763523119290014, 49.81841132577026),\n",
       " (3.1760635004215283, 49.818908012370464),\n",
       " (3.1757765746120548, 49.81940145602188),\n",
       " (3.1754915221885462, 49.81989167789825),\n",
       " (3.1752083309193577, 49.82037869903508),\n",
       " (3.1749269886527047, 49.82086254033053),\n",
       " (3.1746474833161447, 49.821343222546304),\n",
       " (3.174369802916058, 49.821820766308555),\n",
       " (3.1740939355371336, 49.82229519210877),\n",
       " (3.1738198693418567, 49.82276652030463),\n",
       " (3.173547592570002, 49.823234771120916),\n",
       " (3.1732770935381285, 49.82369996465034),\n",
       " (3.1730083606390784, 49.824162120854425),\n",
       " (3.1727413823414796, 49.82462125956438),\n",
       " (3.172476147189249, 49.82507740048192),\n",
       " (3.1722126438011027, 49.82553056318013),\n",
       " (3.1719508608700675, 49.82598076710429),\n",
       " (3.1716907871629947, 49.826428031572725),\n",
       " (3.1714324115200796, 49.826872375777626),\n",
       " (3.17117572285438, 49.827313818785875),\n",
       " (3.1709207101513446, 49.82775237953987),\n",
       " (3.1706673624683357, 49.82818807685831),\n",
       " (3.170415668934163, 49.82862092943706),\n",
       " (3.1701656187486154, 49.82905095584987),\n",
       " (3.169917201181999, 49.829478174549266),\n",
       " (3.1696704055746747, 49.82990260386726),\n",
       " (3.169425221336603, 49.83032426201619),\n",
       " (3.1691816379468882, 49.83074316708947),\n",
       " (3.168939644953327, 49.831159337062395),\n",
       " (3.1686992319719605, 49.83157278979288),\n",
       " (3.168460388686628, 49.83198354302225),\n",
       " (3.1682231048485248, 49.832391614375986),\n",
       " (3.167987370275763, 49.832797021364506),\n",
       " (3.1677531748529337, 49.83319978138389),\n",
       " (3.167520508530673, 49.83359991171663),\n",
       " (3.1672893613252304, 49.83399742953239),\n",
       " (3.167059723318042, 49.834392351888724),\n",
       " (3.166831584655303, 49.83478469573181),\n",
       " (3.1666049355475456, 49.8351744778972),\n",
       " (3.16637976626922, 49.83556171511049),\n",
       " (3.1661560671582762, 49.835946423988105),\n",
       " (3.165933828615748, 49.836328621037964),\n",
       " (3.165713041105343, 49.83670832266021),\n",
       " (3.1654936951530335, 49.837085545147886),\n",
       " (3.1652757813466486, 49.83746030468769),\n",
       " (3.165059290335471, 49.837832617360604),\n",
       " (3.1648442128298364, 49.838202499142625),\n",
       " (3.1646305396007333, 49.83856996590545),\n",
       " (3.1644182614794083, 49.83893503341713),\n",
       " (3.164207369356973, 49.839297717342774),\n",
       " (3.163997854184012, 49.83965803324521),\n",
       " (3.163789706970195, 49.84001599658565),\n",
       " (3.1635829187838898, 49.840371622724355),\n",
       " (3.1633774807517825, 49.84072492692131),\n",
       " (3.1631733840584935, 49.841075924336856),\n",
       " (3.1629706199461998, 49.84142463003234),\n",
       " (3.1627691797142603, 49.84177105897079),\n",
       " (3.162569054718842, 49.842115226017526),\n",
       " (3.162370236372548, 49.842457145940806),\n",
       " (3.162172716144051, 49.84279683341248),\n",
       " (3.1619764855577257, 49.84313430300858),\n",
       " (3.1617815361932853, 49.843469569209994),\n",
       " (3.1615878596854206, 49.84380264640304)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our  m  and  b  values both update with each step. Not only that, but with each step, the size of the changes to  m and  b  decrease. This is because they are approaching a best fit line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's include 2 predictors, $x_1$ and $x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we generated a problem where we have 2 predictors. We generated data such that the best fit line is around $\\hat y = 3x_1 -4x_2 +2$, noting that there is random noise introduced, so the final result will never be exactly that. Let's build what we built previously, but now create a `step_gradient` function that can take an *arbitrary* number of predictors (so the function should be able to include more than 2 predictors as well). Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(11)\n",
    "\n",
    "x1 = np.random.rand(100,1).reshape(100)\n",
    "x2 = np.random.rand(100,1).reshape(100)\n",
    "y_randterm = np.random.normal(0,0.2,100)\n",
    "y = 2+ 3* x1+ -4*x2 + y_randterm\n",
    "\n",
    "data = np.array([y, x1, x2])\n",
    "data = np.transpose(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\n",
    "ax1.set_title('x_1')\n",
    "ax1.plot(x1, y, '.b')\n",
    "ax2.set_title('x_2')\n",
    "ax2.plot(x2, y, '.b');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `step_gradient` function below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, for our gradients, when having multiple predictors $x_j$ with $j \\in 1,\\ldots, k$\n",
    "\n",
    "$$ \\frac{dJ}{dm_j}J(m_j,b) = -2\\sum_{i = 1}^n x_{j,i}(y_i - (\\sum_{j=1}^km{x_{j,i}} + b)) = -2\\sum_{i = 1}^n x_{j,i}*\\epsilon_i$$\n",
    "$$ \\frac{dJ}{db}J(m_j,b) = -2\\sum_{i = 1}^n(y_i - (\\sum_{j=1}^km{x_{j,i}} + b)) = -2\\sum_{i = 1}^n \\epsilon_i $$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we'll have one gradient per predictor along with the gradient for the intercept!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(b_current, m_current ,points):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply 1 step to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply 500 steps to our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the last step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.944428332442866, array([2.995890, -3.911055]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level up - optional\n",
    "\n",
    "Try your own gradient descent algorithm on the Boston Housing data set, and compare with the result from scikit learn!\n",
    "Be careful to test on a few continuous variables at first, and see how you perform. Scikit learn has built-in \"regularization\" parameters to make optimization more feasible for many parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this section, we saw our gradient descent formulas in action.  The core of the gradient descent functions are understanding the two lines: \n",
    "\n",
    "$$ \\frac{dJ}{dm}J(m,b) = -2\\sum_{i = 1}^n x(y_i - (mx_i + b)) = -2\\sum_{i = 1}^n x_i*\\epsilon_i$$\n",
    "$$ \\frac{dJ}{db}J(m,b) = -2\\sum_{i = 1}^n(y_i - (mx_i + b)) = -2\\sum_{i = 1}^n \\epsilon_i $$\n",
    "    \n",
    "Which both look to the errors of the current regression line for our dataset to determine how to update the regression line next.  These formulas came from our cost function, $J(m,b) = \\sum_{i = 1}^n(y_i - (mx_i + b))^2 $, and using the gradient to find the direction of steepest descent.  Translating this into code, and seeing how the regression line continued to improve in alignment with the data, we saw the effectiveness of this technique in practice. Additionally, we saw how you can extend the gradient descent algorithm to multiple predictors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
